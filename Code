# ==============================================================
# Sentiment Analysis of Social Media Data
# ==============================================================

# ========== Step 1. Import Libraries and Load Data ==========
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import re
from collections import Counter

# Load the dataset
file_path ="C:\\Users\\SURIYA\\Downloads\\twitter_training.csv"  # Make sure CSV is in same folder
df = pd.read_csv(file_path)

# Rename columns for clarity
df.columns = ["tweet_id", "topic", "sentiment", "tweet_text"]

print("===== Dataset Loaded Successfully =====")
print("Shape of dataset:", df.shape)
print("\nFirst 5 rows of the dataset:\n", df.head())

# ========== Step 2. Data Cleaning and Understanding ==========
# Check data info
print("\n===== Dataset Information =====")
print(df.info())

# Check for missing values
print("\nMissing values in each column:\n", df.isnull().sum())

# Drop rows where tweet_text is missing
df.dropna(subset=['tweet_text'], inplace=True)

# Clean tweet text: remove URLs, hashtags, mentions, and non-alphabetic characters
def clean_text(text):
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # Remove URLs
    text = re.sub(r"@\w+|#\w+", '', text)                # Remove mentions & hashtags
    text = re.sub(r"[^A-Za-z\s]", '', text)              # Keep only letters
    text = text.lower().strip()
    return text

df['cleaned_text'] = df['tweet_text'].apply(clean_text)

# Verify cleaning
print("\n===== Cleaned Data Sample =====")
print(df[['tweet_text', 'cleaned_text']].head())

# Check for missing values
print("\nMissing values in each column:\n", df.isnull().sum())

# ========== Step 3. Full EDA and Visualizations ==========

# 3.1 Sentiment distribution
sentiment_counts = df['sentiment'].value_counts()
print("\n===== Sentiment Distribution =====")
print(sentiment_counts)

plt.figure(figsize=(8, 5))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette="viridis")
plt.title("Overall Sentiment Distribution", fontsize=16)
plt.xlabel("Sentiment")
plt.ylabel("Number of Tweets")
plt.show()

# 3.2 Tweet length analysis
df['tweet_length'] = df['cleaned_text'].apply(len)
plt.figure(figsize=(8, 5))
sns.histplot(df['tweet_length'], bins=30, kde=True, color='blue')
plt.title("Distribution of Tweet Length", fontsize=16)
plt.xlabel("Tweet Length")
plt.ylabel("Frequency")
plt.show()

# 3.3 Sentiment by Topic/Brand
topic_sentiment = df.groupby(['topic', 'sentiment']).size().unstack().fillna(0)
print("\n===== Sentiment by Topic =====")
print(topic_sentiment.head())

topic_sentiment.plot(kind='bar', stacked=True, figsize=(12, 6), colormap="viridis")
plt.title("Sentiment Distribution by Topic/Brand", fontsize=16)
plt.xlabel("Topic/Brand")
plt.ylabel("Number of Tweets")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 3.4 Word Cloud for all tweets
stop_words = set(STOPWORDS)
all_words = " ".join(df['cleaned_text'])
wc = WordCloud(stopwords=stop_words, background_color='white', width=800, height=400).generate(all_words)

plt.figure(figsize=(10, 6))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.title("Overall Word Cloud", fontsize=16)
plt.show()

# ========== Step 4. Sentiment Pattern Analysis ==========

# 4.1 Word Clouds for each sentiment
def plot_wordcloud(sentiment):
    text = " ".join(tweet for tweet in df[df['sentiment'] == sentiment]['cleaned_text'])
    wc = WordCloud(stopwords=stop_words,
                   background_color='white',
                   width=800,
                   height=400,
                   colormap='plasma').generate(text)
    plt.figure(figsize=(8, 6))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Word Cloud for {sentiment} Tweets", fontsize=14)
    plt.show()

for sentiment in df['sentiment'].unique():
    plot_wordcloud(sentiment)

# 4.2 Top keywords for each sentiment
def get_top_n_words(sentiment, n=10):
    words = " ".join(df[df['sentiment'] == sentiment]['cleaned_text']).split()
    words = [word for word in words if word not in stop_words]
    return Counter(words).most_common(n)

for sentiment in df['sentiment'].unique():
    top_words = get_top_n_words(sentiment, 10)
    words_df = pd.DataFrame(top_words, columns=['Word', 'Frequency'])

    plt.figure(figsize=(8, 5))
    sns.barplot(data=words_df, x='Frequency', y='Word', palette="coolwarm")
    plt.title(f"Top 10 Words in {sentiment} Tweets", fontsize=14)
    plt.show()

# 4.3 Sentiment ratio per topic
topic_sentiment_ratio = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0)
topic_sentiment_ratio.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='coolwarm')
plt.title("Sentiment Ratio by Topic/Brand", fontsize=16)
plt.xlabel("Topic/Brand")
plt.ylabel("Proportion of Sentiments")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ========== Step 5. Insights ==========
print("\n===== INSIGHTS =====")
print("1. Negative tweets dominate the dataset, followed closely by positive tweets.")
print("2. Sentiment distribution by brand highlights areas of satisfaction or dissatisfaction.")
print("3. Word clouds reveal trending topics and emotions in tweets.")
print("4. Businesses can leverage this data to understand customer feedback and improve strategies.")


